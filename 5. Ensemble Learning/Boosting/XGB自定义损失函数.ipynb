{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. XGBoost的损失函数必须为二阶可导的凸函数\n",
    "- 海塞矩阵必为正定矩阵，因为XGB需要计算二阶偏导数的累计平方和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, mean_squared_error, accuracy_score, recall_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, n_classes=2, random_state=2018)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y, test_size=0.25, random_state=2018)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=8, n_targets=1, random_state=2018)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.25, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.以ln(cosh(x))为损失函数\n",
    "https://baike.baidu.com/item/cosh/3535060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:1133.472053\n",
      "AUC:0.922540, F1:0.825397, Acc:0.824000\n"
     ]
    }
   ],
   "source": [
    "# 定义函数\n",
    "def log_cosh_obj(y_true, y_pred):\n",
    "    delta = y_pred - y_true \n",
    "    grad = np.tanh(delta)\n",
    "    hess = (1.0 - grad*grad)\n",
    "    return grad, hess\n",
    "\n",
    "# 回归问题\n",
    "model = XGBRegressor(objective=log_cosh_obj)\n",
    "model.fit(X_train_reg, y_train_reg)\n",
    "y_pred = model.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred)\n",
    "print('MSE:{:.6f}'.format(mse))\n",
    "\n",
    "# 分类问题\n",
    "model = XGBClassifier(objective=log_cosh_obj)\n",
    "model.fit(X_train_clf, y_train_clf)\n",
    "y_pred, y_prob = model.predict(X_test_clf), model.predict_proba(X_test_clf)[:, -1]\n",
    "auc = roc_auc_score(y_test_clf, y_prob)\n",
    "f1 = f1_score(y_test_clf, y_pred)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print('AUC:{:.6f}, F1:{:.6f}, Acc:{:.6f}'.format(auc, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pseudo-Huber loss function，可以近似替代MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:1111.974434\n",
      "AUC:0.928238, F1:0.830040, Acc:0.828000\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "def huber_approx_obj(y_true, y_pred, h=1):\n",
    "    # h为Pseudo-Huber loss function中的参数，用于调节坡度，其值越大，图像越陡峭\n",
    "    d = y_pred - y_true \n",
    "    scale = 1 + np.square(d / h)\n",
    "    scale_sqrt = np.sqrt(scale)\n",
    "    grad = d / scale_sqrt\n",
    "    hess = 1 / scale / scale_sqrt\n",
    "    return grad, hess\n",
    "\n",
    "# 回归问题\n",
    "model = XGBRegressor(objective=huber_approx_obj)\n",
    "# model = XGBRegressor(objective='reg:linear')\n",
    "model.fit(X_train_reg, y_train_reg)\n",
    "y_pred = model.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred)\n",
    "print('MSE:{:.6f}'.format(mse))\n",
    "\n",
    "# 分类问题\n",
    "model = XGBClassifier(objective=huber_approx_obj)\n",
    "model.fit(X_train_clf, y_train_clf)\n",
    "y_pred, y_prob = model.predict(X_test_clf), model.predict_proba(X_test_clf)[:, -1]\n",
    "auc = roc_auc_score(y_test_clf, y_prob)\n",
    "f1 = f1_score(y_test_clf, y_pred)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print('AUC:{:.6f}, F1:{:.6f}, Acc:{:.6f}'.format(auc, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 以log(exp(-x) + exp(x))为损失函数：处理分类问题较合适"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.922540, F1:0.825397, Acc:0.824000\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "def log_exp(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    t1 = np.exp(d) - np.exp(-d) \n",
    "    t2 = np.exp(d) + np.exp(-d) \n",
    "    grad = t1 / t2\n",
    "    hess = 1.0 - grad**2 \n",
    "    return grad, hess\n",
    "\n",
    "# 分类问题\n",
    "model = XGBClassifier(objective=log_exp)\n",
    "model.fit(X_train_clf, y_train_clf)\n",
    "y_pred, y_prob = model.predict(X_test_clf), model.predict_proba(X_test_clf)[:, -1]\n",
    "auc = roc_auc_score(y_test_clf, y_prob)\n",
    "f1 = f1_score(y_test_clf, y_pred)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print('AUC:{:.6f}, F1:{:.6f}, Acc:{:.6f}'.format(auc, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fair Loss：意义不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:2724600341.150338\n",
      "AUC:0.208725, F1:0.263566, Acc:0.240000\n"
     ]
    }
   ],
   "source": [
    "def fair_obj(y_true, y_pred, c=1):\n",
    "    x = y_true - y_pred\n",
    "    den = np.abs(x) + c\n",
    "    grad = c*x / den\n",
    "    hess = c*c / np.square(den) \n",
    "    return grad, hess\n",
    "\n",
    "model = XGBRegressor(objective=fair_obj, n_estimators=500, learning_rate=0.15)\n",
    "# model = XGBRegressor(objective='reg:linear')\n",
    "model.fit(X_train_reg, y_train_reg)\n",
    "y_pred = model.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred)\n",
    "print('MSE:{:.6f}'.format(mse))\n",
    "\n",
    "\n",
    "model = XGBClassifier(objective=fair_obj, n_estimators=500, learning_rate=0.15)\n",
    "model.fit(X_train_clf, y_train_clf)\n",
    "y_pred, y_prob = model.predict(X_test_clf), model.predict_proba(X_test_clf)[:, -1]\n",
    "auc = roc_auc_score(y_test_clf, y_prob)\n",
    "f1 = f1_score(y_test_clf, y_pred)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print('AUC:{:.6f}, F1:{:.6f}, Acc:{:.6f}'.format(auc, f1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 自定义函数：适合处理分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lodd(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    grad = np.log1p(d) \n",
    "    hess = 0.5 / (abs(d)+0.001) \n",
    "    return grad, hess\n",
    "\n",
    "model = XGBClassifier(objective=lodd)\n",
    "model.fit(X_train_clf, y_train_clf)\n",
    "y_pred, y_prob = model.predict(X_test_clf), model.predict_proba(X_test_clf)[:, -1]\n",
    "auc = roc_auc_score(y_test_clf, y_prob)\n",
    "f1 = f1_score(y_test_clf, y_pred)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print('AUC:{:.6f}, F1:{:.6f}, Acc:{:.6f}'.format(auc, f1, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
