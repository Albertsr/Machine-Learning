- **Author：** 马肖
- **E-mail：** maxiaoscut@aliyun.com
- **Github：**  https://github.com/Albertsr

---

## 1. PCA

#### 1.1 PCA概述
- PCA的目标在于找到一个投影矩阵W，使得原数据从高维空间降维到低维空间，同时尽可能保留更多的信息

- PCA本质上是一个有损的特征压缩过程，我们期望压缩的过程中满足两点：
  - 降维后各维度上的数据点尽可能地分散，即方差越大则保留的信息越多。
  - 两个不同维度间关联度越小越好，最理想就是两个维度的相关系数为0，在线性空间内表现为两个维度正交。

---

#### 1.2 PCA过程
- 将原矩阵去中心化（每一列都减去该列的均值，使得每列的均值为0）、归一化（即每一列都除以该列的标准差），得到矩阵A
  
- 求矩阵A的协方差矩阵，X_cov = X.T * X / (m - ddof), 其中m为样本数，ddof一般取1，表示无偏估计
  
- 对协方差矩阵进行相似对角化 （协方差矩阵为对称矩阵，对称矩阵必能正交化）
  
- 在协方差矩阵相似对角化的过程中，非对角线上的元素（**即各特征之间的协方差**）全部化为0，对角线上元素为协方差矩阵的特征值，同时也是各列特征的方差
  
- 选取协方差矩阵最大的前k（k<n）个特征值对应的k个特征向量（列向量）构成的矩阵记为W，则原矩阵右乘矩阵W，则原矩阵由n维度降维至k维

---

#### 1.3 PCA的注意事项

- PCA是无监督学习，没有考虑feature和label的关系，不能用于防止过拟合
- PCA的假设是方差越大信息量越多。但是信息（方差）小的特征并不代表对学习器没有意义，可能某些方差小的特征对学习器的性能仍有显著影响，因此**PCA降维可能会导致一些关键但方差小的信息被过滤掉**

---

## 2. Kernel PCA
#### 1. 问题的引出

- Kernel PCA通过非线性映射将数据从低维特征空间映射到高维空间，在高维空间中再使用传统的PCA将其映射到另一个低维空间中

- Kernel PCA有助于把流形“展开”，使得对于在低维空间难以线性分类的数据点，在更高维度上能找到合适的线性分类平面

---

#### 2. 数学推导


#### 3. 通过核函数直接求解高维映射后的样本在主成分上的投影  

- 对映射后的矩阵`$X$`进行主成分分析，实际上是计算`$X$`的行向量`$\phi(x_i)$`在主成分上的投影，即`$\phi(x_i)*v$`，但是`$\phi(x_i)$`以及 `$v=X^Tu/\sqrt{\lambda}$`中的`$X^T$`需要显示定义映射函数`$\phi$`的具体表达式。
  
- **通过核函数直接求解映射后的样本在主成分上的投影**
  - 映射函数的显式定义是无需甚至无法求出的，我们关心的只是投影后的内积
  - 核技巧可以避免显式定义映射函数，直接求解投影后的内积


## 3. SVD

- **Author：** 马肖
- **E-mail：** maxiaoscut@aliyun.com
- **Github：**  https://github.com/Albertsr

---

## SVD 

## 1. 特征值分解公式

- 对于任意一个`$m*n$`型矩阵`$A$`，都可以写成：`$A = U_{m*m}S_{m*n}V_{n*n}^T$` 
- `$U$`是`$m$`阶**正交矩阵**，它的每一列都是`$m$`阶半正定矩阵`$AA^T$`的特征向量，也称为矩阵`$A$`的**左奇异向量**.
- `$V$`是`$n$`阶**正交矩阵**，它的每一列都是`$n$`阶半正定矩阵`$A^TA$`的特征向量，也称为矩阵`$A$`的**右奇异向量**.
- `$S$`是`$m*n$`型奇异值矩阵，除了对角线上是奇异值外，其余位置都是零。奇异值为非负值，在矩阵`$S$`主对角线上降序排列

---

## 2. 奇异值的求解

`$A = USV^T \implies AV = US \implies Av_i = \sigma_i u_i 
\implies \sigma_i = Av_i / u_i $`

其中`$v_i$`是`$V$`的列向量，`$u_i$`是`$U$`的列向量，`$\sigma_i$`是`$S$`的第`$i$`个取值

---

## 3. 重要结论
#### 结论1：奇异值`$\sigma_i$`的平方等于`$AA^T$`的特征值，即`$\sigma_i^2 = \lambda_i$`

#### 结论2：`$AA^T$`的特征向量为矩阵`$U$`的列向量，`$A^TA$`的特征向量为矩阵`$V$`的列向量
`$AA^T = USV^TVS^TU^T = USS^TU^T = US^2U^T \implies$`
正交矩阵`$U$`将矩阵`$AA^T$`对角化，`$m$`阶对角矩阵`$S^2$`主对角线上的元素为`$AA^T$`的特征值，`$U$`的列向量为`$AA^T$`的特征向量

#### 结论3：`$AA^T$`与`$A^TA$`均为半正定矩阵，则特征值`$\lambda \geq 0$`, 奇异值`$\sigma \geq 0$`

- 任意一个`$m$`维列向量`$x$`, `$x^TAA^Tx = \alpha^T \alpha \geq 0$`,其中`$\alpha = A^Tx$`,因此`$AA^T$`为半正定矩阵,同理可证`$A^TA$`为半正定矩阵

---

## 4. SVD的优势

#### 4.1 SVD可用于数据压缩和去燥
`$A_{m*m} = U_{m*m}S_{m*n}V^T_{n*n} \approx U_{m*k}S_{k*k}V^T_{k*n}$`

#### 4.2 SVD也可用于PCA降维
- 传统PCA是通过协方差矩阵的特征值分解(EVD)实现的，即根据`$X^TX / (n-1)$` 求协方差矩阵，然后选取协方差矩阵最大的几个特征值对应的特征向量构成投影矩阵用于降维。它的缺点在于：
  - `$X^TX$`可能使得`$X$`中的某些非常小的数在矩阵相乘的过程中丢失；
  - `$X$`较大时，`$X^TX$`较耗时
- SVD不需要求协方差矩阵也能得到右奇异矩阵`$V$`从而避免了上述两个缺点 
#### 4.3 SVD可以很方便地求解矩阵的伪逆矩阵

- `$A^{-1} = VS^{-1}U^{T}$`
- **备注：** 奇异矩阵或非方阵的矩阵不存在逆矩阵，若存在与A的转置矩阵`$A^T$`同型的矩阵`$X$`，并且满足：`$AXA=A, XAX=X$`，则称矩阵`$X$`为矩阵`$A$`的伪逆
